{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUdOjIQyjA09gqf+jePQZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SubhashishM/GENAI/blob/main/Assignment_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Twitter Data\n"
      ],
      "metadata": {
        "id": "Jfc9LmQIuvkf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sqxu7xbqgaZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "Ifeqk1icqsLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "3FCVvXD1rBVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "d3jJCuzUrWl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "X4uyCAVkrmvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thoughtvector/customer-support-on-twitter"
      ],
      "metadata": {
        "id": "nFWgz1dMr-py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/customer-support-on-twitter.zip"
      ],
      "metadata": {
        "id": "1U8tD4DDtBap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#df = pd.read_csv('/content/twcs/twcs.csv')\n",
        "df = pd.read_csv('/content/sample.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "Wm1cyyZEtxBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Extraction"
      ],
      "metadata": {
        "id": "hXNVndAwu16_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = ' '.join(str(item) for item in df['text'])"
      ],
      "metadata": {
        "id": "mqHpVWizSHHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textblob\n",
        "from textblob import TextBlob\n",
        "txtblob=TextBlob(data)\n",
        "txtblob.correct().string"
      ],
      "metadata": {
        "id": "t67wT1o7vUfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if word not in stopwords.words(\"english\"):\n",
        "      new_text.append(word)\n",
        "  return \" \".join(new_text)\n",
        "\n",
        "remove_stopwords(data)"
      ],
      "metadata": {
        "id": "W29cQFlhwDZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_url(text):\n",
        "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(\"\",text)\n",
        "\n",
        "data_without_url = remove_url(data)\n",
        "data_without_url"
      ],
      "metadata": {
        "id": "xp-AmqNCVZEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_at(text):\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if \"@\" not in word:\n",
        "      new_text.append(word)\n",
        "  return \" \".join(new_text)\n",
        "\n",
        "data_without_url_at = remove_at(data_without_url)\n",
        "data_without_url_at"
      ],
      "metadata": {
        "id": "MYtX_IBjWY2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punc(text):\n",
        "    for char in string.punctuation:\n",
        "        text=text.replace(char,\"\")\n",
        "    return text\n",
        "\n",
        "data_without_url_at_punc = remove_punc(data_without_url_at)\n",
        "data_without_url_at_punc"
      ],
      "metadata": {
        "id": "cj5ZNcxCU_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import emoji\n",
        "import re\n",
        "def remove_emojis_manually(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    clean_text = emoji_pattern.sub(r'', text)\n",
        "    return clean_text\n",
        "\n",
        "data_without_url_at_punc_emoji = remove_emojis_manually(data_without_url_at_punc)\n",
        "data_without_url_at_punc_emoji"
      ],
      "metadata": {
        "id": "wiff6D0exoCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "word_tokenize(data_without_url_at_punc_emoji)"
      ],
      "metadata": {
        "id": "AqDrUbf-yksM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(data_without_url_at_punc_emoji)"
      ],
      "metadata": {
        "id": "xd1bYU_yzV_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stemming and lemmetization"
      ],
      "metadata": {
        "id": "11v3Ms_xz7Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stemming(text):\n",
        "    obj=PorterStemmer()\n",
        "\n",
        "    stem_word=[obj.stem(word) for word in text.split()]\n",
        "\n",
        "    return stem_word"
      ],
      "metadata": {
        "id": "4XZck0BP0AvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming(data_without_url_at_punc_emoji)"
      ],
      "metadata": {
        "id": "tPw1jJgi0FXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lammatization(text):\n",
        "    words=text.split()\n",
        "\n",
        "    lemmetizer=WordNetLemmatizer()\n",
        "\n",
        "    lemetized_word=[lemmetizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return lemetized_word"
      ],
      "metadata": {
        "id": "CGL_NkSV0fT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lammatization(data_without_url_at_punc_emoji)"
      ],
      "metadata": {
        "id": "xcHdZPDG0hOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "mPSv4QJOtLaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']"
      ],
      "metadata": {
        "id": "EV3GpXjLmsF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Instantiate the OneHotEncoder\n",
        "encoder = OneHotEncoder(max_categories=10, sparse=False)\n",
        "\n",
        "# Fit and transform the categorical column\n",
        "one_hot_encoded = encoder.fit_transform(df[['text']])\n",
        "\n",
        "# Display the result\n",
        "pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(['text']))"
      ],
      "metadata": {
        "id": "t9olXAbTmDYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "BOW=CountVectorizer()\n",
        "document_matrix=BOW.fit_transform(df[\"text\"])"
      ],
      "metadata": {
        "id": "SEl_bb1dhGyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_matrix[6].toarray()"
      ],
      "metadata": {
        "id": "icTKUXpFhssV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOW.vocabulary_"
      ],
      "metadata": {
        "id": "X3bdAZLKh54V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram=CountVectorizer(ngram_range=(2,2))\n",
        "bigramvocab=bigram.fit_transform(df[\"text\"])\n",
        "bigram.vocabulary_"
      ],
      "metadata": {
        "id": "oN9ptWisiLOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram=CountVectorizer(ngram_range=(3,3))\n",
        "bigramvocab=bigram.fit_transform(df[\"text\"])\n",
        "bigram.vocabulary_"
      ],
      "metadata": {
        "id": "QkQpP5vkieUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram=CountVectorizer(ngram_range=(1,3))\n",
        "bigramvocab=bigram.fit_transform(df[\"text\"])\n",
        "bigram.vocabulary_"
      ],
      "metadata": {
        "id": "UqbWDDfqizuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "tfidf.fit_transform(df[\"text\"]).toarray()"
      ],
      "metadata": {
        "id": "vo1dNbC4i_PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.get_feature_names_out()"
      ],
      "metadata": {
        "id": "8CWdARXkjOTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.idf_"
      ],
      "metadata": {
        "id": "1bjQ6_axjStV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OHE vs BOW vs N-grams vs TF-IDF"
      ],
      "metadata": {
        "id": "gWWGhyGFvMTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***One-Hot Encoding (OHE):***\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Simple Representation: OHE is a straightforward and easy-to-understand representation, where each category is represented by a binary indicator.\n",
        "2. No Assumption of Relationships: OHE doesn't assume any relationships between categories. Each category is treated independently.\n",
        "3. Useful for Categorical Data: OHE is particularly useful for dealing with categorical data when there is no ordinal relationship between categories.\n",
        "4. Interpretability: The resulting representation is interpretable, making it easy to understand the contribution of each category to the model.\n",
        "5. No Parameter Tuning: OHE does not involve tuning parameters, making it easy to implement and use in various models.\n",
        "**Disadvantages**\n",
        "1. High Dimensionality: OHE can lead to a high-dimensional representation, especially when dealing with a large number of categories, which can be computationally expensive.\n",
        "2. Sparse Representation: The resulting matrix is often sparse, containing many zeros, which can be inefficient for storage and computation.\n",
        "3. Loss of Order Information: OHE does not capture any ordinal relationship between categories. All categories are treated as equal.\n",
        "4. May Not Handle Large Vocabularies Well: When dealing with large vocabularies, OHE can become impractical due to the resulting high-dimensional space.\n",
        "5. Collinearity Issues: OHE can introduce multicollinearity when used in linear models if one category is a perfect predictor of another.\n",
        "---\n",
        "***Bag-of-Words (BOW):***\n",
        "\n",
        "**Advantages:**\n",
        "1. Simple and Intuitive: BOW is a simple and intuitive representation that captures the frequency of words in a document.\n",
        "2. Versatile: BOW can be used for various natural language processing (NLP) tasks, such as text classification, sentiment analysis, and information retrieval.\n",
        "3. Easy to Implement: Implementing BOW is straightforward and can be done with minimal preprocessing.\n",
        "4. Robust to Noise: BOW can be robust to noise in the data, as it focuses on the occurrence of words rather than their order.\n",
        "5. Scalability: BOW can handle large datasets efficiently and is scalable.\n",
        "**Disadvantages:**\n",
        "1. Loss of Sequence Information: BOW disregards the order and sequence of words, which may result in a loss of important context.\n",
        "2. No Semantics: BOW does not capture semantic relationships between words.\n",
        "3. Fixed-Length Representation: BOW represents documents as fixed-length vectors, which may not be suitable for variable-length documents.\n",
        "4. Sparse Representation: Similar to OHE, BOW can lead to a sparse representation with many zero entries.\n",
        "5. Ignores Word Importance: BOW treats all words equally, ignoring variations in importance.\n",
        "---\n",
        "***N-grams:***\n",
        "**Advantages:**\n",
        "1. Captures Local Context: N-grams capture local context by considering sequences of adjacent words.\n",
        "2. Handles Variable-Length Context: N-grams allow for a variable-length context, capturing different levels of context based on the chosen N.\n",
        "3. Retains Some Order Information: While not capturing full syntactic structure, N-grams retain some order information by considering adjacent word pairs.\n",
        "4. Applicable to Multiple Tasks: N-grams can be applied to various NLP tasks, including language modeling, sentiment analysis, and named entity recognition.\n",
        "5. Combines Simplicity with Improved Context: Compared to BOW, N-grams provide improved context information while maintaining simplicity.\n",
        "**Disadvantages:**\n",
        "1. Exponential Growth in Vocabulary Size: The vocabulary size can grow exponentially with the order of N, leading to increased computational complexity.\n",
        "2. Sparse Representation: Like BOW, N-grams can result in a sparse representation, especially with large vocabularies.\n",
        "3. May Capture Noise: Longer N-grams may capture noise or spurious patterns that do not contribute to the underlying meaning.\n",
        "4. Limited Understanding of Global Context: N-grams have limitations in understanding global context or long-range dependencies in text.\n",
        "5. Dependent on Corpus Size: The effectiveness of N-grams can depend on the size of the training corpus.\n",
        "\n",
        "---\n",
        "***TF-IDF (Term Frequency-Inverse Document Frequency):***\n",
        "\n",
        "**Advantages:**\n",
        "1. Weighting for Importance: TF-IDF assigns weights to words based on their importance in a document and across the corpus.\n",
        "2. Handles Common Words: TF-IDF mitigates the issue of common words by down-weighting terms that appear frequently across documents.\n",
        "3. Promotes Discriminative Features: TF-IDF emphasizes terms that are discriminative for a specific document.\n",
        "4. Reduces Noise: By down-weighting common terms, TF-IDF reduces the impact of noise in the data.\n",
        "5. Applicable to Various NLP Tasks: TF-IDF is versatile and can be used in tasks such as text classification, information retrieval, and clustering.\n",
        "**Disadvantages:**\n",
        "1. Ignores Word Order: TF-IDF, like BOW, ignores word order and does not capture syntactic or semantic relationships.\n",
        "2. Sparse Representation: The resulting TF-IDF matrix can be sparse, especially with a large vocabulary.\n",
        "3. Doesn't Consider Document Length: TF-IDF does not explicitly consider the length of documents, which may be important in certain contexts.\n",
        "4. Sensitive to Tokenization: The effectiveness of TF-IDF can be sensitive to the chosen tokenization method.\n",
        "5. May Require Tuning: Tuning parameters, such as the choice of stop words and the smoothness factor for IDF, may be required for optimal performance."
      ],
      "metadata": {
        "id": "0ASTnVWCs_pP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dw9xQti5jWDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}